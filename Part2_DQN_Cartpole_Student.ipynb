{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B07uZUvvYA1l",
        "colab_type": "text"
      },
      "source": [
        "# DQN with Pytorch\n",
        "\n",
        "You will use Pytorch to create a DQN for the cartpole gym example. \n",
        "\n",
        "Let's import the necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# Google Colab\n",
        "### Rendering Dependancies\n",
        "If you prefer you can use Google Colab to prevent your CPU from overheating while training, if you don't have a GPU:\n",
        "\n",
        "1. Open this notebook in Colab https://colab.research.google.com/\n",
        "2. Run the following snippet there\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this for google colab\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# We will use matplot to plot our progress during training\n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import collections\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukx17b9HzG8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7YKI2thZIrw",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network\n",
        "The neural network in this case is pretty simple. We are using fully conntected layers. \n",
        "Of course it could be replaced by something more complex like a CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyoeHn8zloto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, img_height, img_width):\n",
        "        super().__init__()\n",
        "            \n",
        "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)   \n",
        "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
        "        self.out = nn.Linear(in_features=32, out_features=2)\n",
        "        \n",
        "    def forward(self, t):\n",
        "        t = t.flatten(start_dim=1)\n",
        "        t = F.relu(self.fc1(t))\n",
        "        t = F.relu(self.fc2(t))\n",
        "        t = self.out(t)\n",
        "        return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AccbDcGbVWM",
        "colab_type": "text"
      },
      "source": [
        "# Replay Memory\n",
        "We will use a deque as our replay memory. A deque automatically removes the first element after appending an Experience to the end if the max lenghth is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFjBJnLNlpLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Experience = collections.namedtuple(\n",
        "    'Experience',\n",
        "    (\n",
        "        'state', \n",
        "        'action', \n",
        "        'next_state', \n",
        "        'reward', \n",
        "        'done' # we also store if the episode was completed after taking the step\n",
        "    )\n",
        ")\n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = collections.deque(maxlen=capacity)  \n",
        "    def append(self, experience):\n",
        "        self.memory.append(experience)\n",
        "    # ranomly select experiences from memory of batch_size \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll8sbAqLaePM",
        "colab_type": "text"
      },
      "source": [
        "# Screen\n",
        "This are some helper functions to obtain a processed current screen from the environment. \n",
        "\n",
        "`get_screen()` returns a 4D tensor as (Batch, Color-Channel, Height, Width), which is a necessary order for Pytorch. \n",
        "\n",
        "Another reason for processing is to make training faster. We crop the whitespaces from the screen and get smaller images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXTO97Gyu1Ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen(env, device):\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # Strip off the edges, so that we have a square image centered on a cart\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v94S8TzCdVj4",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll define some helper functions for plotting our progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bknuuts_x6-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(values, moving_avg_period):\n",
        "    plt.figure(2)\n",
        "    plt.clf()        \n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(values)\n",
        "\n",
        "    moving_avg = get_moving_average(moving_avg_period, values)\n",
        "    plt.plot(moving_avg)    \n",
        "    plt.pause(0.001)\n",
        "    print(\"Episode\", len(values), \"\\n\", \\\n",
        "        moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
        "    if is_ipython: display.clear_output(wait=True)\n",
        "\n",
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUH9lvsve6BV",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters\n",
        "We set up the hyperparameters. \n",
        "\n",
        "Notice that the number of episodes `num_episodes` is really low because traing takes some time. \n",
        "\n",
        "If you have a GPU and some time at home you can try it out with a higher number like `1000`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SWFTGtHyyNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "gamma = 0.999\n",
        "\n",
        "# Needed for our epsilon-greedy-method\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "\n",
        "target_update = 10\n",
        "\n",
        "memory_size = 100000\n",
        "lr = 0.001\n",
        "\n",
        "num_episodes = 120 # Set to a higher number like 1000 at home when you have more time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rw6wZVDm_Am",
        "colab_type": "text"
      },
      "source": [
        "# Initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OQZ-qG2o9Jh",
        "colab_type": "text"
      },
      "source": [
        "We set up the device which will be used by pytorch during training. If you have a CUDA GPU then it will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR25-Brsn6Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtQsVqxQpOzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0').unwrapped\n",
        "env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK9OrKem2A8T",
        "colab_type": "text"
      },
      "source": [
        "Let's use our `get_screen()` method to have a look at our cart pole environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7M6p0_L2Fb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "screen = get_screen(env, device)\n",
        "screen = env.render('rgb_array')\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(screen)\n",
        "plt.title('Non-processed screen example')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufjI_2n8pPfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory = ReplayMemory(memory_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0rnOeasn7Zc",
        "colab_type": "text"
      },
      "source": [
        "We need the screen height and width to initialize our both networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzYwEn-Opo02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "screen = get_screen(env, device)\n",
        "_, _, screen_height, screen_width = screen.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks_3z2lfpZyP",
        "colab_type": "text"
      },
      "source": [
        "Next we set up the policy network and a target network as copy of the policy network. To do that we need the height and width of a single screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ6B-ja90wb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy_net = DQN(screen_height, screen_width).to(device)\n",
        "target_net = DQN(screen_height, screen_width).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hYKYCsIph4o",
        "colab_type": "text"
      },
      "source": [
        "Using Pytorch the learnable parameters like weights and biases are under the model's parameters.\n",
        "\n",
        "A state_dict is a dictionary object that maps each layer to its parameter tensor. We copy the state_dict of the policy_net to the target_net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Uo3-Hfpkdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_net.load_state_dict(policy_net.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4etSg3Qp8jC",
        "colab_type": "text"
      },
      "source": [
        "We will set up the target_net for evaluation only. We only do train the policy_net. The target_net is updated manually later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD8cK19Yp7Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_net.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esTxPkzJqF4S",
        "colab_type": "text"
      },
      "source": [
        "We will use Adam for the optimization process. It combines AdaGrad and RMSProp:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4UVT-zYqEp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqR3QuOrfW0",
        "colab_type": "text"
      },
      "source": [
        "# Some python particularities\n",
        "Before we will start with the training let's analyse some python specific things.\n",
        "\n",
        "## Use * to pass arguments to a function\n",
        "When putting the asteriks * at the beginning of an iterable (A list is an iterable for example) to pass all elements of the iterable to a function each as a separate argument. This is very usefull when you don't know how many elements a list has for example.\n",
        "\n",
        "Have a look at the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFzlj1NzrolM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = [\"hi\", \"what's\", \"up?\"]\n",
        "print(test)\n",
        "\n",
        "# the following examples are both equivalent:\n",
        "print(test[0], test[1], test[2])\n",
        "print(*test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGdep8GZrzFa",
        "colab_type": "text"
      },
      "source": [
        "## zip\n",
        "Zip takes for example multiple lists and outputs a zip object, which is an iterator of tuples. The elements with the same index of the different iterables are put toghether as seperate tuples in the new list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAmDSB4Pr2St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = [\"a\", \"b\", \"c\"]\n",
        "b = [\"1\", \"2\", \"3\"]\n",
        "\n",
        "x = zip(a, b) # contains zip object with [(a,1), (b,2), (c,3)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgJ_UGPPr5JK",
        "colab_type": "text"
      },
      "source": [
        "To unpack the content of the zip object we can use the asteriks again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHiEAh5Lr6s9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(*x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FXNx2Zdr8Yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = [ # batches of experiences\n",
        "        ( # first batch element \n",
        "            [1, 2, 3], # state\n",
        "            2, # action\n",
        "            [6, 5, 4], # next_state\n",
        "            4, #reward\n",
        "            False\n",
        "        ), \n",
        "        ( # second batch element\n",
        "            [5, 4, 3], # state\n",
        "            0, # action\n",
        "            [6, 5, 4], # next_state\n",
        "            4, #reward,\n",
        "            False\n",
        "        ), \n",
        "        ( # third batch element\n",
        "            [2, 1, 2], # state\n",
        "            0, # action\n",
        "            [5, 10, 9], # next_state\n",
        "            4, #reward\n",
        "            True\n",
        "        ), \n",
        "]\n",
        "\n",
        "e = Experience(*zip(*a))\n",
        "print(\"experience of batch-arrays\", e)\n",
        "\n",
        "states, actions, next_states, rewards, dones = e\n",
        "print(\"states\", states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzWm_IL4r_cg",
        "colab_type": "text"
      },
      "source": [
        "Notice how all states of all batch elements are now under the same tuple state. The same is true for the actions, next_states and rewards.\n",
        "\n",
        "We will use that later in our algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwwGeB3I4o7O",
        "colab_type": "text"
      },
      "source": [
        "# Difference of the last two frames\n",
        "\n",
        "To make it easier for the network to differentiate between states we will create a state as the difference of the last two frames later. Let's have a look at the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AlE89FL3vZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frame1 = get_screen(env, device)\n",
        "frame2 = get_screen(env, device)\n",
        "\n",
        "difference = frame1 - frame2\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(difference.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
        "plt.title('Starting state example')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WqhHqHk5xhi",
        "colab_type": "text"
      },
      "source": [
        "In this case we get a black screen, because frame1 and frame2 are the same.\n",
        "\n",
        "Let's reset the environment in-between. This should get us a non black image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjUw1of45-kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frame1 = get_screen(env, device)\n",
        "env.reset()\n",
        "frame2 = get_screen(env, device)\n",
        "\n",
        "difference = frame1 - frame2\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(difference.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
        "plt.title('Starting state example')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuMCdizq6MUj",
        "colab_type": "text"
      },
      "source": [
        "Now we can see only the parts that are truly different. This will make it easier for the network to take the movement into account. Those states are what we feed the network with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI6PX35VrPs8",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "1. Have a look at the training algorithm first\n",
        "2. Try to implement the epsilon greedy algorithmn to select an action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7dALpNwzddO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used to calculate the epsilon value\n",
        "total_steps = 0\n",
        "\n",
        "# necessary for plotting\n",
        "episode_durations = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    env.reset()\n",
        "\n",
        "    # In the beginning the screen is black\n",
        "    current_screen = get_screen(env, device)\n",
        "    black_screen = torch.zeros_like(current_screen)\n",
        "\n",
        "    state = black_screen # start with black screen\n",
        "\n",
        "    for timestep in count():\n",
        "      # take action depending on epsilon\n",
        "      epsilon = ... # calculate epsilon\n",
        "      total_steps += 1\n",
        "      if epsilon > random.random():\n",
        "        action = random.randrange(env.action_space.n)\n",
        "        action = torch.tensor([action]).to(device) # explore\n",
        "      else:\n",
        "          with torch.no_grad():\n",
        "            action = policy_net(state).argmax(dim=1).to(device) # exploit\n",
        "\n",
        "      _, reward, done, _ = env.step(action.item())\n",
        "      reward = torch.tensor([reward], device=device)\n",
        "\n",
        "      next_state = None\n",
        "      if not done:\n",
        "        s1 = current_screen\n",
        "        s2 = get_screen(env, device)\n",
        "        current_screen = s2\n",
        "\n",
        "        # The next state is the difference of the frames s2 and s1\n",
        "        next_state = s2 - s1\n",
        "      else:\n",
        "        # black screen if we are done\n",
        "        next_state = black_screen\n",
        "      \n",
        "      is_done = torch.tensor([done], dtype=torch.bool, device=device)\n",
        "      memory.append(Experience(state, action, next_state, reward, is_done))\n",
        "      state = next_state\n",
        "\n",
        "      if memory.can_provide_sample(batch_size):\n",
        "        experiences = memory.sample(batch_size)\n",
        "\n",
        "        states, actions, next_states, rewards, dones = Experience(*zip(*(experiences)))\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        next_states = torch.cat(next_states)\n",
        "        rewards = torch.cat(rewards)\n",
        "        done_mask = torch.cat(dones)\n",
        "        \n",
        "        # calculate Q(s,a)\n",
        "        q_values = policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "\n",
        "        # calculate Q_target(s_a)\n",
        "        next_q_values = target_net(next_states).detach() # detach --> no gradient will be backproped for next_q_values\n",
        "        max_next_q_values = next_q_values.max(dim=1).values\n",
        "        # target = r if episode ended, that's why we set to 0 for states after which episode ended\n",
        "        max_next_q_values[done_mask] = 0.0 #  next_states after which are done should NOT be considered\n",
        "        \n",
        "        # target = r + y * maxQ_target(s', a')\n",
        "        target = rewards + gamma * max_next_q_values  \n",
        "\n",
        "        loss = F.mse_loss(q_values, target.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      if done:\n",
        "        episode_durations.append(timestep)\n",
        "        plot(episode_durations, 100)\n",
        "        break\n",
        "      if episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "      env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU55rAIaeF9i",
        "colab_type": "text"
      },
      "source": [
        "# References: \n",
        "Pytorch Documentation: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "DeepLizard: https://deeplizard.com/\n",
        "\n",
        "HandsOnReinforcement Learning: https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter06"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DQN_Cartpole_FZI_Student.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}